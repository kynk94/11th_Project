{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess by 서석현.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kynk94/11th_Project/blob/master/dance/preprocess_by_%EC%84%9C%EC%84%9D%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH6AZ3c-Gb5s",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess for Everybody dance now - by 서석현\n",
        "\n",
        "1. preparation   \n",
        "  * 영상 2개(target, source)\n",
        "  * target이 바꾸고 싶은 영상(ex 내 모습)이라고 생각하면 되고 이 친구를 학습시켜야함\n",
        "  * source는 춤추는 영상 \n",
        "  * Realtime, Multi person pose estimation에 대한 파일이 필요 - git clone으로 극복  \n",
        "  \n",
        "2. summary of the code\n",
        "  * google drive 마운트 시키기! (쉽게 말하면 colab이랑 구글 드라이브랑 연동하기)\n",
        "  * 우선 동영상 다운 받기 - colab에서 잘 안돌아가서 로컬에서 다운 받기 추천\n",
        "  * cv2로 동영상을 읽어서 프레임으로 쪼개기 - 주로 1초에 20~30 프레임 정도 나옴\n",
        "  * png 파일에 대해서 pose -estimation하기\n",
        "  \n",
        "3. Recommendation  \n",
        "  * 로컬이 아닌 colab에서 돌리길 추천합니다.\n",
        "  * 컴퓨터가 좋으면 사실 상관 없습니다. 그냥 돌려도 괜찮습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K8DGtcKafGd5",
        "outputId": "b14d9a9d-fa96-4c54-e0f5-fca7ae0ecc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# 구글 드라이브 연동 시키기 -> 링크에 나와 있는 친구 복사해서 붙여넣기 하면 됨\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR5f1o4kV2aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "#from pytube import YouTube as youtube #유투브에서 동영상 다운받기\n",
        "from pathlib import Path\n",
        "\n",
        "drive = Path('/gdrive/My Drive/source/target') # 제 구글 드라이브에 폴더 만드는 작업입니다.\n",
        "drive.mkdir(exist_ok=True)\n",
        "images = drive.joinpath('images')\n",
        "images.mkdir(exist_ok=True)\n",
        "\n",
        "#yt = youtube('https://www.youtube.com/watch?v=4wpn3y66mzY') # 이 친구는 로컬에서 돌리는 걸 추천\n",
        "#yt.streams.first().download(drive, 'mv') # mv.mp4로 이름 지어줍니다.\n",
        "\n",
        "cap = cv2.VideoCapture(str(drive.joinpath('mv-2.mp4'))) # 이건 이미 제가 받은 친구들 프레임으로 자르는 과정입니다.\n",
        "i = 0\n",
        "while(cap.isOpened()): # 열려 있을 때까지\n",
        "    flag, frame = cap.read() # 동영상을 읽고 (읽는 방식이 프레임로 읽는다고 생각하면 편합니다.)\n",
        "    if not(flag) or i == 1000: # 1000까지만 읽음 -> 늘려도 괜찮\n",
        "        break\n",
        "    cv2.imwrite(str(images.joinpath(f'img_{i:04d}.png')), frame) #img_0001.png로 저장\n",
        "    i += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xrwUne1Hym4",
        "colab_type": "text"
      },
      "source": [
        "### Pose Estimation from git clone\n",
        "\n",
        "이 친구를 사용해서 사진 하나에서 사람이 어떤 포즈를 하고 있는지 묘사\n",
        "\n",
        " * pretrained data needed - check the file pose_model.pth\n",
        " * 팀 깃허브에 다운로드 링크 공유해둠\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNC9J1roixoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-2_HSjVywVu",
        "colab_type": "code",
        "outputId": "62331dad-c14a-4c77-c6b3-9ec65c602543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd pytorch_Realtime_Multi-Person_Pose_Estimation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch_Realtime_Multi-Person_Pose_Estimation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipxC5_7Wy9Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from network.rtpose_vgg import get_model # pose-estimation에서 옴\n",
        "from evaluate.coco_eval import get_multiplier, get_outputs # pose-estimation에서 옴\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "from skimage import filters\n",
        "from scipy import ndimage\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih8_OiJMKIqt",
        "colab_type": "text"
      },
      "source": [
        "아래 코드는 다음과 같은 주소에서 따왔습니다.\n",
        "\n",
        "각 함수에 대해서 간략하게 소개를 하자면 \n",
        "\n",
        "1. remove_noise:   \n",
        "input : 이미지  \n",
        "output: argmax를 통한 나머지 흑백 처리 -> 예전에 제가 올려드린 그 검은색 사진 생각하시면 될거 같습니다.  \n",
        "\n",
        "2. 라벨 만들기 -> 이 부분은 어려운데 걍 뼈다귀 만드는 과정인거 같긴합니다. 정확히 표현하면 뼈다귀 joint 점을 찾는 과정인거 같아용\n",
        "\n",
        "3. get_pose : 검은색 사진에 흰색 뼈다귀가 나온다고 생각하시면 편합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyic2ZwhzNA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# openpose reference https://github.com/CUHKSZ-TQL/EverybodyDanceNow_reproduce_pytorch/src\n",
        "\n",
        "from network.post import * #pose-estimation에서 옴\n",
        "\n",
        "def remove_noise(img):\n",
        "    th = filters.threshold_otsu(img)\n",
        "    bin_img = img > th\n",
        "    regions, num = ndimage.label(bin_img)\n",
        "    areas = []\n",
        "    for i in range(num):\n",
        "        areas.append(np.sum(regions == i+1))\n",
        "    img[regions != np.argmax(areas)+1] = 0\n",
        "    return img\n",
        "\n",
        "\n",
        "def create_label(shape, joint_list, person_to_joint_assoc):\n",
        "    label = np.zeros(shape, dtype=np.uint8)\n",
        "    cord_list = []\n",
        "    for limb_type in range(17):\n",
        "        for person_joint_info in person_to_joint_assoc:\n",
        "            joint_indices = person_joint_info[joint_to_limb_heatmap_relationship[limb_type]].astype(int)\n",
        "            if -1 in joint_indices:\n",
        "                continue\n",
        "            joint_coords = joint_list[joint_indices, :2]\n",
        "            coords_center = tuple(np.round(np.mean(joint_coords, 0)).astype(int))\n",
        "            cord_list.append(joint_coords[0])\n",
        "            limb_dir = joint_coords[0, :] - joint_coords[1, :]\n",
        "            limb_length = np.linalg.norm(limb_dir)\n",
        "            angle = math.degrees(math.atan2(limb_dir[1], limb_dir[0]))\n",
        "            polygon = cv2.ellipse2Poly(coords_center, (int(limb_length / 2), 4), int(angle), 0, 360, 1)\n",
        "            cv2.fillConvexPoly(label, polygon, limb_type+1)\n",
        "    return label,cord_list\n",
        "\n",
        "\n",
        "def get_pose(param, heatmaps, pafs):\n",
        "    shape = heatmaps.shape[:2]\n",
        "    # Bottom-up approach:\n",
        "    # Step 1: find all joints in the image (organized by joint type: [0]=nose,\n",
        "    # [1]=neck...)\n",
        "    joint_list_per_joint_type = NMS(param, heatmaps)\n",
        "    # joint_list is an unravel'd version of joint_list_per_joint, where we add\n",
        "    # a 5th column to indicate the joint_type (0=nose, 1=neck...)\n",
        "    joint_list = np.array([tuple(peak) + (joint_type,) for joint_type,\n",
        "                           joint_peaks in enumerate(joint_list_per_joint_type) for peak in joint_peaks])\n",
        "\n",
        "    # Step 2: find which joints go together to form limbs (which wrists go\n",
        "    # with which elbows)\n",
        "    paf_upsamp = cv2.resize(pafs, shape, interpolation=cv2.INTER_CUBIC)\n",
        "    connected_limbs = find_connected_joints(param, paf_upsamp, joint_list_per_joint_type)\n",
        "\n",
        "    # Step 3: associate limbs that belong to the same person\n",
        "    person_to_joint_assoc = group_limbs_of_same_person(connected_limbs, joint_list)\n",
        "\n",
        "    # (Step 4): plot results\n",
        "    label,cord_list = create_label(shape, joint_list, person_to_joint_assoc)\n",
        "\n",
        "    return label, cord_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA7wr-hvzRui",
        "colab_type": "code",
        "outputId": "8a091890-9c71-48c4-ef6d-a3d938dba8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 저장되어 있는 pose-estimation 모델 불러와서 cuda 친구로 보내주기\n",
        "import torch\n",
        "weight_name = '/gdrive/My Drive/source/pose_model.pth'\n",
        "model = get_model('vgg19')     \n",
        "model.load_state_dict(torch.load(weight_name))\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "model.float()\n",
        "model.eval()\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bulding VGG19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRSy5oLbz16K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#제가 아까 저장했던 사진들에 대한 경로를 지정해줍시다\n",
        "\n",
        "save_dir = Path('/gdrive/My Drive/source/target')\n",
        "img_dir = Path('/gdrive/My Drive/source/target/images')\n",
        "'''make label images for pix2pix'''\n",
        "#test_img는 원본 파일, #test_label_ori는 포즈가 나오는 검은 사진, # test_head_ori는 머리부분만 추출 -face gan에 사용 -> 논문 참조\n",
        "test_img_dir = save_dir.joinpath('test_img')\n",
        "test_img_dir.mkdir(exist_ok=True)\n",
        "test_label_dir = save_dir.joinpath('test_label_ori')\n",
        "test_label_dir.mkdir(exist_ok=True)\n",
        "test_head_dir = save_dir.joinpath('test_head_ori')\n",
        "test_head_dir.mkdir(exist_ok=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ec8xB-zVBnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 자 사진을 받아서 포즈를 만들어봅시다.\n",
        "from tqdm import tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "pose_cords = []\n",
        "for idx in tqdm_notebook(range(len(os.listdir(str(img_dir))))):\n",
        "    img_path = img_dir.joinpath('img_{:04}.png'.format(idx)) # 아까 저장된 사진을 읽어봅시다.\n",
        "    img = cv2.imread(str(img_path))\n",
        "    shape_dst = np.min(img.shape[:2])\n",
        "    oh = (img.shape[0] - shape_dst) // 2\n",
        "    ow = (img.shape[1] - shape_dst) // 2\n",
        "\n",
        "    img = img[oh:oh + shape_dst, ow:ow + shape_dst]\n",
        "    img = cv2.resize(img, (512, 512)) # resize\n",
        "    multiplier = get_multiplier(img)\n",
        "    with torch.no_grad():\n",
        "        paf, heatmap = get_outputs(multiplier, img, model, 'rtpose') # estimate 모델을 통해서 국수 뽑듯이 뽑음\n",
        "    r_heatmap = np.array([remove_noise(ht)\n",
        "                          for ht in heatmap.transpose(2, 0, 1)[:-1]]) \\\n",
        "        .transpose(1, 2, 0)\n",
        "    heatmap[:, :, :-1] = r_heatmap\n",
        "    param = {'thre1': 0.1, 'thre2': 0.05, 'thre3': 0.5}\n",
        "    label, cord = get_pose(param, heatmap, paf)\n",
        "    index = 13\n",
        "    crop_size = 25\n",
        "    try:\n",
        "        head_cord = cord[index]\n",
        "    except:\n",
        "        head_cord = pose_cords[-1] # if there is not head point in picture, use last frame\n",
        "\n",
        "    pose_cords.append(head_cord)\n",
        "    head = img[int(head_cord[1] - crop_size): int(head_cord[1] + crop_size),\n",
        "           int(head_cord[0] - crop_size): int(head_cord[0] + crop_size), :]\n",
        "    plt.imshow(head)\n",
        "    plt.savefig(str(test_head_dir.joinpath('pose_{}.jpg'.format(idx))))\n",
        "    plt.clf()\n",
        "    cv2.imwrite(str(test_img_dir.joinpath('{:05}.png'.format(idx))), img) # 저장은 00001.png로 \n",
        "    cv2.imwrite(str(test_label_dir.joinpath('{:05}.png'.format(idx))), label)\n",
        "    if idx % 100 == 0 and idx != 0:\n",
        "        pose_cords_arr = np.array(pose_cords, dtype=np.int)\n",
        "        np.save(str((save_dir.joinpath('pose_source.npy'))), pose_cords_arr) # 1000장에 대한 포즈 정보를 저장하는 것 같습니다.\n",
        "\n",
        "pose_cords_arr = np.array(pose_cords, dtype=np.int)\n",
        "np.save(str((save_dir.joinpath('pose_target.npy'))), pose_cords_arr)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1iUAEfNNbx",
        "colab_type": "text"
      },
      "source": [
        "위 코드를 돌리면 1000장에 약 1시간 걸립니다..\n",
        "\n",
        "실험 환경\n",
        "\n",
        "Tesla T4\n",
        "\n",
        "1000장\n",
        "\n",
        "자 이제 우리는 학습으로 넘어갑시다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjYVtLDZNxdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}